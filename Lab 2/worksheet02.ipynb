{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 2\n",
    "## Bayesian inference\n",
    "\n",
    "***\n",
    "\n",
    "In this part of the workshop, we'll develop some intuition for priors and posteriors, which are crucial to Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "from scipy.stats import bernoulli, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A lucky find\n",
    "\n",
    "On the way to class, you discover an unusual coin on the ground.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/1_2_penny_Middlesex_DukeYork_1795_1ar85_%288737903267%29.jpg\" alt=\"Coin\" width=\"350\"/>\n",
    "\n",
    "As a dedicated student in statistical ML, you're interested in determining whether the coin is _biased_. \n",
    "More specifically, you want to estimate the probability $\\theta$ that the coin will land heads-up when you toss it. If $\\theta \\approx \\frac{1}{2}$ then we say that the coin is _unbiased_ (or fair).\n",
    "\n",
    "You can use the function below to simulate a coin toss: it returns `1` for heads and `0` for tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toss_coin():\n",
    "    if bernoulli.rvs(p = (int.from_bytes(\"coin\".encode(), 'little') % 10000)/10000):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior belief\n",
    "Before you even toss the coin, you notice that the heads side appears to have more mass. \n",
    "Thus, your _prior belief_ is that $\\theta$ is slightly biased away from $\\frac{1}{2}$ towards 0â€”i.e. you expect tails are more likely.\n",
    "\n",
    "To quantify this prior belief, we assume that the prior distribution for $\\theta$ is $\\mathrm{Beta}(a,b)$, for some choice of the hyperparameters $a, b > 0$. \n",
    "(See [link](https://en.wikipedia.org/wiki/Beta_distribution) for info about the Beta distribution.)\n",
    "The prior probability density function for $\\theta$ is therefore given by:\n",
    "\n",
    "$$ p(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1 - \\theta)^{b-1} $$\n",
    "\n",
    "where $B(a,b)$ is a special function called the _Beta function_.\n",
    "\n",
    "Select appropriate values for $a$ and $b$ by looking at the plot of $p(\\theta)$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ... # fill in\n",
    "b = ... # fill in\n",
    "theta = np.linspace(0, 1, 1001)\n",
    "plt.plot(theta, beta.pdf(theta, a, b))\n",
    "plt.title('Prior distribution')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior updates\n",
    "Now toss the coin once and denote the outcome by $x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = toss_coin()\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our belief about $\\theta$, based on this new evidence $x_1$.\n",
    "To do this we apply Bayes' rule to compute the posterior for $\\theta$:\n",
    "\n",
    "$$ p(\\theta | x_1) = \\frac{p(x_1 | \\theta) \\, p(\\theta)}{p(x_1)} \\propto p(x_1 | \\theta) \\, p(\\theta)$$\n",
    "\n",
    "where $p(\\theta)$ is the prior given above and \n",
    "\n",
    "$$ p(x_1 | \\theta) = \\theta^{x_1} (1 - \\theta)^{1 - x_1} $$\n",
    "\n",
    "is the likelihood.\n",
    "\n",
    "***\n",
    "**Exercise:** Show (on paper) that\n",
    "\n",
    "$$ p(\\theta | x_1) \\propto \\theta^{x_1 + a - 1} (1 - \\theta)^{(1 - x_1) + b - 1} $$\n",
    "\n",
    "which implies that $\\theta | x_1 \\sim \\mathrm{Beta}[x_1 + a, (1 - x_1) + b]$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toss the coin a second time, denoting the outcome by $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = toss_coin()\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we want to update our belief about $\\theta$ based on the new information $x_2$. \n",
    "We take the previous posterior $p(\\theta|x_1)$ as the new prior and apply Bayes' rule:\n",
    "\n",
    "$$p(\\theta | x_1, x_2) \\propto p(x_2 | \\theta) p(\\theta | x_1)$$\n",
    "\n",
    "\\[Note: We assume the tosses are independent, otherwise the likelihood for $x_2$ would depend on $x_1$.\\] \n",
    "This gives $\\theta | x_1, x_2 \\sim \\mathrm{Beta}[x_1 + x_2 + a, (2 - x_1 - x_2) + b]$.\n",
    "\n",
    "***\n",
    "**Exercise:** Show that for $n$ coin tosses, the posterior is $\\theta | x_1, \\ldots, x_n \\sim \\operatorname{Beta}[n_H + a, n - n_H + b]$ where $n_H = \\sum_{i = 1}^{n} x_i$ is the number of heads observed.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MAP estimator and MLE estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior $\\theta|x_1, \\ldots, x_n$ contains all the information we know about $\\theta$ after observing $n$ coin tosses.\n",
    "One way of obtaining a point estimate of $\\theta$ from the posterior, is to take the value with the maximum a posteriori probability (MAP):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{\\theta}_\\mathrm{MAP} &= \\arg \\max_{\\theta} p(\\theta|x_1, \\ldots, x_n) \\\\\n",
    "        & = \\frac{n_H + a - 1}{n + a + b - 2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In general, the MAP estimator gives a different result to the maximum likelihood estimator (MLE) for $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{\\theta}_\\mathrm{MLE} &=\\arg \\max_{\\theta} p(x_1, \\ldots, x_n|\\theta) \\\\\n",
    "        & = \\frac{n_H}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "**Exercise:** How would you derive the above results for $\\hat{\\theta}_\\mathrm{MAP}$ and  $\\hat{\\theta}_\\mathrm{MLE}$? Setup the equations necessary to solve for $\\hat{\\theta}_\\mathrm{MAP}$ and  $\\hat{\\theta}_\\mathrm{MLE}$. You do not need to solve the equations at this stage.\n",
    "\n",
    "**Extension** (Only return to this if you have completed the remaining workshop): Solve the equations you derived above. Give the condition for the estimators to be (exactly) equal, i.e. $\\hat{\\theta}_\\mathrm{MAP} \\equiv \\hat{\\theta}_\\mathrm{MLE}$. What is the prior in this case?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convergence of the estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now toss the coin an additional 48 times (so that $n = 50$), recording $\\hat{\\theta}_\\mathrm{MLE}$ and $\\hat{\\theta}_\\mathrm{MAP}$ after each toss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tosses = 48\n",
    "num_tosses = 2 + extra_tosses\n",
    "num_heads = 0\n",
    "theta_map = np.zeros(num_tosses)\n",
    "theta_mle = np.zeros(num_tosses)\n",
    "for i in range(0, num_tosses):\n",
    "    if i == 0: \n",
    "        num_heads += x1 \n",
    "    elif i == 1:\n",
    "        num_heads += x2\n",
    "    else:\n",
    "        num_heads += toss_coin()\n",
    "    theta_map[i] = ... # fill in\n",
    "    theta_mle[i] = ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_map, label = \"MAP\")\n",
    "plt.plot(theta_mle, label = \"MLE\")\n",
    "plt.xlabel('Number of draws')\n",
    "plt.ylabel(r'$\\hat{\\theta}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** \n",
    "\n",
    "1. Is the coin biased?\n",
    "1. Do the MAP and MLE estimates converge to the same value for $\\theta$?\n",
    "1. What happens if you set $a = 1; b = 1$?\n",
    "1. How does the posterior distribution for $\\theta$ compare to the prior plotted above? (Use the code block below to plot the posterior.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dist = beta(a + num_heads, b + num_tosses - num_heads)\n",
    "plt.plot(theta, theta_dist.pdf(theta))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta|x_1, \\ldots, x_n)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll visualize the evolution of the posterior distribution as we observe more data. Before running the code cell below, take a couple of minutes to discuss with those around you how you expect the posterior to behave qualitatively) as the number of observed samples $x_n$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://matplotlib.org/3.1.1/gallery/animation/bayes_update.html\n",
    "\n",
    "class UpdateBetaBernoulli:\n",
    "    def __init__(self, ax, a, b, theta_num_points = 201):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.ax = ax\n",
    "        self.num_heads = 0\n",
    "        self.num_tosses = 0\n",
    "        self.theta = np.linspace(0, 1, theta_num_points)\n",
    "        self.line, = ax.plot([], [])\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset\"\"\"\n",
    "        self.num_heads = 0\n",
    "        self.num_tosses = 0\n",
    "        self.line.set_data([], [])\n",
    "        return self.line,\n",
    "\n",
    "    def __call__(self, num_tosses):\n",
    "        \"\"\"Perform tosses and update plot\"\"\"\n",
    "        for _ in range(num_tosses):\n",
    "            self.num_tosses += 1\n",
    "            self.num_heads += toss_coin()\n",
    "        y = beta.pdf(self.theta, self.num_heads + self.a, self.num_tosses - self.num_heads + self.b)\n",
    "        self.line.set_data(self.theta, y)\n",
    "        self.ax.set_title('{:>4} heads, {:>4} tosses'.format(self.num_heads, self.num_tosses), family='monospace')\n",
    "        return self.line, self.ax.title\n",
    "\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 15)\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_ylabel(r'$p(\\theta|x_1, \\ldots)$')\n",
    "\n",
    "ud = UpdateBetaBernoulli(ax, a, b)\n",
    "FuncAnimation(fig, ud, frames=[1]*200, init_func=ud.reset, repeat=False, interval=50, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Bonus material: Bayesian credible intervals\n",
    "In principle, the posterior distribution contains all the information about the possible values of the parameter $\\theta$. To show the utility of the posterior, we can obtain a quantitative measure of the posterior uncertainty by computing a central (or equal-tailed) interval of posterior probability. These are known as _Bayesian credible intervals_ and should not be confused with the frequentist concept of _confidence intervals_ which leverage the distribution of point estimators. For a Bayesian credible interval, an e.g. 95% credible interval contains the true parameter value with 95% probability. In general, for a $1- \\alpha$ interval, where $\\alpha \\in (0,1)$, this corresponds to the range of values $I = (\\theta_1, \\theta_2)$ above and below which lie exactly $\\alpha/2$ of the posterior probability. That is, $\\alpha/2$ of the probability mass of the posterior lies below $\\theta_1$, and $\\alpha/2$ of the probability mass lies above $\\theta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05  # define the confidence level\n",
    "theta_1, theta_2 = theta_dist.ppf([alpha/2., 1-alpha/2.])  # Inverse of the CDF - returns relevant quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check that $1-\\alpha$ of the probability mass actually lies inside our computed interval. That is, we expect \n",
    "\n",
    "$$ \\int_{\\theta_1}^{\\theta_2} d \\theta \\; p(\\theta \\vert x_1, \\ldots x_n) = 1-\\alpha $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "integrate.quad(lambda x: theta_dist.pdf(x), a=theta_1, b=theta_2)  # second return value gives absolute error in integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! What does this interval look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pdf = theta_dist.pdf(theta)\n",
    "pdf_line, = plt.plot(theta, theta_pdf)\n",
    "plt.title(r'Posterior - $\\theta_\\mathrm{MAP} =$' + ' ${:.3f}$'.format(theta_map[-1]))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta|x_1, \\ldots, x_N)$')\n",
    "plt.vlines(x=theta_1, ymin=0, ymax=theta_dist.pdf(theta_1), linestyle='--', color=pdf_line.get_color())\n",
    "plt.vlines(x=theta_2, ymin=0, ymax=theta_dist.pdf(theta_2), linestyle='--', color=pdf_line.get_color())\n",
    "plt.fill_between(theta, theta_pdf, 0, where=(theta > theta_1) & (theta < theta_2), alpha=0.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
