{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COMP90051 Workshop 4\n",
    "## Logistic regression\n",
    "\n",
    "***\n",
    "\n",
    "In this worksheet, we'll implement logistic regression from scratch using the iteratively reweighted least-squares (IRLS) algorithm presented in lectures. \n",
    "In doing so, we'll see how logistic regression can be solved by iteratively performing weighted linear regression.\n",
    "Finally, we'll compare our IRLS implementation with gradient descent.\n",
    "\n",
    "Firstly, we'll import the relevant libraries (`numpy`, `matplotlib`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 108\n",
    "RND_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Binary classification data\n",
    "\n",
    "Let's begin by generating some binary classification data.\n",
    "To make it easy for us to visualise the results, we'll stick to a two-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples = 20, n_features = 2, n_informative=2, n_redundant=0, random_state=RND_SEED)\n",
    "X_b = np.column_stack((np.ones_like(y), X))\n",
    "\n",
    "plt.scatter(X[y==0,0], X[y==0,1], color='b', label=\"$y = 0$\")\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color='r', label=\"$y = 1$\")\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "**Question:** What do you notice about the data? It is possible for logistic regression to achieve perfect accuracy on this data?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Weighted linear regression\n",
    "\n",
    "Before we can implement the IRLS algorithm for logistic regression, we'll need a function that can solve _weighted_ linear regression problems. \n",
    "This is a minor extension of the material we covered in last week's worksheet.\n",
    "Whereas last week, each sample contributed equally to the empirical risk, we now allow the samples to contribute with different weights.\n",
    "Specifically, the empirical risk becomes:\n",
    "\n",
    "$$\n",
    "\\hat{R}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^{n} m_i (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2\n",
    "$$\n",
    "\n",
    "where $m_i > 0$ is the weight of sample $i$. We assume the feature vectors $\\mathbf{x}_i$ have been prepended with \"1\" entries to incorporate a bias term.\n",
    "\n",
    "_\\[Aside: weighted linear regression can be used in applications where each sample has a different variance $\\sigma_i^2$ (e.g. measurement error). The best linear unbiased estimator is obtained by setting $m_i = \\frac{1}{\\sigma_i^2}$ assuming the variances are known.\\]_\n",
    "\n",
    "Weighted linear regression can also be solved analytically, by generalising the normal equation as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\star = (\\mathbf{X}^\\top \\mathbf{M} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{M} \\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_1^\\top \\\\ \\vdots \\\\ \\mathbf{x}_n^\\top \\end{pmatrix}$, $\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$ and $\\mathbf{M} = \\operatorname{diag}(m_1, \\ldots, m_n)$.\n",
    "\n",
    "Below we define a function `fit_linear` that fits a weighted linear model given training data $\\mathbf{X}$, $\\mathbf{y}$ and sample weights $\\mathbf{m} = [m_1, \\ldots, m_n]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit_linear(X, y, sample_weight = None):\n",
    "    \"\"\"Fits a linear regression model according to the given training data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Feature matrix. The matrix must contain a constant column to \n",
    "        incorporate a non-zero bias.\n",
    "        \n",
    "    y : array of shape (n_samples,)\n",
    "        Response relative to X\n",
    "    \n",
    "    sample_weight : numpy.ndarray of shape (n_samples,) default=None\n",
    "        Weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weights : array of shape (n_features,)\n",
    "    \"\"\"\n",
    "    if sample_weight is not None:\n",
    "        sqrt_sample_weight = np.sqrt(sample_weight)\n",
    "        X = X * sqrt_sample_weight[:, np.newaxis]\n",
    "        y = y * sqrt_sample_weight\n",
    "    weights, _, _, _ = np.linalg.lstsq(X.T @ X, X.T @ y, rcond=None)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Logistic regression via IRLS\n",
    "\n",
    "We're now ready to start implementing the IRLS algorithm for logistic regression step-by-step.\n",
    "\n",
    "Recall that logistic regression assumes a linear relationship between the features $\\mathbf{x}$ and the log-odds of the event $Y = 1$:\n",
    "\n",
    "$$\n",
    "\\log \\frac{p(y = 1|\\mathbf{x})}{1 - p(y = 1|\\mathbf{x})} = \\mathbf{x}^\\top \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "From a decision-theoretic point of view, we choose the weights vector $\\mathbf{w}$ to minimise the empirical risk under the log-loss (a.k.a. cross-entropy loss and logistic loss):\n",
    "\n",
    "$$\n",
    "\\hat{R}(\\mathbf{w}) = - \\frac{1}{n} \\sum_{i = 1}^{n} \\ell_\\mathrm{log}(y_i, \\mu_i(\\mathbf{w})),\n",
    "$$\n",
    "\n",
    "where $\\ell_\\mathrm{log}(y, \\mu) = y \\log \\mu + (1 - y) \\log (1 - \\mu)$ and  $\\mu_i(\\mathbf{w}) := \\frac{1}{1 + \\exp( - \\mathbf{x}_i^\\top \\mathbf{w})}$.\n",
    "\n",
    "We'll need to evaluate $\\hat{R}(\\mathbf{w})$ later on to generate convergence plots, so we define a function for this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def risk(X, y, w):\n",
    "    \"\"\"Evaluate the empirical risk under the cross-entropy (logistic) loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Feature matrix. The matrix must contain a constant column to \n",
    "        incorporate a non-zero bias.\n",
    "        \n",
    "    y : array of shape (n_samples,)\n",
    "        Response relative to X. Binary classes must be encoded as 0 and 1.\n",
    "    \n",
    "    w : array of shape (n_features,)\n",
    "        Weight vector.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    risk : float\n",
    "    \"\"\"\n",
    "    mu = expit(X @ w)\n",
    "    return log_loss(y, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll also need to be able to compute the _gradient_ of the empirical risk $\\nabla_{\\mathbf{w}} \\hat{R}(\\mathbf{w}')$ in order to:\n",
    "\n",
    "* decide when we can stop the IRLS algorithm (we're close to optimality when $\\|\\nabla_{\\mathbf{w}} \\hat{R}(\\mathbf{w}')\\|_\\infty \\approx 0$)\n",
    "* implement gradient descent (later) as an alternative to IRLS\n",
    "\n",
    "It's straightforward to show (using vector calculus) that:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\hat{R}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^{n} (\\mu_i(\\mathbf{w}) - y_i)\\mathbf{x}_i = \\frac{1}{n} \\mathbf{X}^\\top (\\boldsymbol{\\mu} - \\mathbf{y}),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu} = \\begin{pmatrix} \\mu_1(\\mathbf{w}) \\\\ \\vdots \\\\ \\mu_n(\\mathbf{w}) \\end{pmatrix}$.\n",
    "\n",
    "***\n",
    "**Exercise:** Complete the `grad_risk` function below, which computes $\\nabla_{\\mathbf{w}} \\hat{R}(\\mathbf{w})$ for a given weight vector $\\mathbf{w}$ and training data $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def grad_risk(X, y, w):\n",
    "    \"\"\"Evaluate the gradient of the empirical risk\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Feature matrix. The matrix must contain a constant column to \n",
    "        incorporate a non-zero bias.\n",
    "        \n",
    "    y : array of shape (n_samples,)\n",
    "        Response relative to X. Binary classes must be encoded as 0 and 1.\n",
    "    \n",
    "    w : array of shape (n_features,)\n",
    "        Weight vector.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad_w : array of shape (n_features,)\n",
    "    \"\"\"\n",
    "    grad_w = ... # fill in\n",
    "    return grad_w\n",
    "\n",
    "# Test case\n",
    "if RND_SEED == 0:\n",
    "    test_grad_risk_actual = grad_risk(X_b, y, np.ones(3))\n",
    "    test_grad_risk_desired = np.array([0.11641865, -0.25260051, 0.20606407])\n",
    "    np.testing.assert_allclose(test_grad_risk_actual, test_grad_risk_desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In lectures, we saw that the IRLS algorithm solves logistic regression by iteratively solving a series of weighted linear regression problems.\n",
    "\n",
    "At iteration $t$, the weights vector $\\mathbf{w}_t$ is updated by solving the normal equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t + 1} = (\\mathbf{X}^\\top \\mathbf{M}_t \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{M}_t \\mathbf{b}_t,\n",
    "$$\n",
    "\n",
    "where \n",
    "* $\\mathbf{b}_t = \\mathbf{X} \\mathbf{w}_t + \\mathbf{M}_t^{-1}(\\mathbf{y} - \\boldsymbol{\\mu}_t)$ is the linearised response (analogue of $\\mathbf{y}$),\n",
    "* $\\mathbf{M}_t = \\operatorname{diag}(\\boldsymbol{\\mu}_t (1 - \\boldsymbol{\\mu}_t))$ are the sample weights, and\n",
    "* $\\boldsymbol{\\mu}_t = [\\mu_{1}(\\mathbf{w}_t), \\ldots, \\mu_{n}(\\mathbf{w}_t)]^\\top$.\n",
    "\n",
    "***\n",
    "**Exercise:** Complete the `update_weight_irls` function below, which performs a single IRLS weight update.\n",
    "\n",
    "_Hint: you should use the previously-defined `fit_linear` function to solve the normal equation._\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_weight_irls(X, y, w):\n",
    "    \"\"\"Performs a weight update using the IRLS algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Feature matrix. The matrix must contain a constant column to \n",
    "        incorporate a non-zero bias.\n",
    "    \n",
    "    y : array of shape (n_samples,)\n",
    "        Response relative to X. Binary classes must be encoded as 0 and 1.\n",
    "    \n",
    "    w : array of shape (n_features,)\n",
    "        Current estimate of the weight vector.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array of shape (n_features,)\n",
    "        Updated estimate of the weights vector.\n",
    "    \"\"\"\n",
    "    # Compute the sample weights and linearised response\n",
    "    # fill in\n",
    "    \n",
    "    # Fit a weighted linear regression model\n",
    "    w = ... # fill in\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Test case\n",
    "if RND_SEED == 0:\n",
    "    test_update_weight_irls_actual = update_weight_irls(X_b, y, np.ones(3))\n",
    "    test_update_weight_irls_desired = np.array([0.88475769, 2.08819846, 0.04110822])\n",
    "    np.testing.assert_allclose(test_update_weight_irls_actual, test_update_weight_irls_desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've implemented `grad_risk` and `update_weight_irls`, we're very close to being able to run IRLS.\n",
    "We just need to write some code to:\n",
    "\n",
    "* initialise the weight vector, and\n",
    "* iterate until the stopping criterion $\\| \\nabla_\\mathbf{w} \\hat{R}(\\mathbf{w}_t) \\|_\\infty \\leq \\mathtt{tol}$ is satisfied (or a max number of iterations is completed).\n",
    "\n",
    "We do this in the `fit_logistic` function below. \n",
    "Note that we treat the `update_weight` function as a parameter, which defaults to IRLS. \n",
    "This will allow us to reuse `fit_logistic` for gradient descent later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit_logistic(X, y, w_init=None, max_iter = 100, tol = 1e-4, \n",
    "                 update_weight = update_weight_irls, **kwargs):\n",
    "    \"\"\"Fits a binary logistic regression model according to the given training \n",
    "    data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Feature matrix. The matrix must contain a constant column to \n",
    "        incorporate a non-zero bias.\n",
    "        \n",
    "    y : array of shape (n_samples,)\n",
    "        Response relative to `X`. Binary classes must be encoded as 0 and 1.\n",
    "    \n",
    "    w_init : array of shape (n_features,) default=None\n",
    "        Initial guess for the weights vector. Defaults to a vector of\n",
    "        zeroes.\n",
    "    \n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations\n",
    "    \n",
    "    tol : float, default=1e-4\n",
    "        Stop when the inf-norm of the gradient falls below this value.\n",
    "    \n",
    "    update_weight : callable, default=update_weight_irls\n",
    "        Callable that performs a weight update. Must have signature \n",
    "        (X, y, w, **kwargs). Defaults to IRLS.\n",
    "    \n",
    "    **kwargs : \n",
    "        Keyword arguments passed to `update_weight`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w_history : list of arrays of shape (n_features,)\n",
    "        History of weight vectors\n",
    "    \"\"\"\n",
    "    if w_init is None:\n",
    "        # Default weight initialisation\n",
    "        w_init = np.zeros(X.shape[1], dtype=float)\n",
    "        \n",
    "    # Store history of weights\n",
    "    w_history = [w_init]\n",
    "    w = w_init\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        w = update_weight(X, y, w, **kwargs)\n",
    "        w_history.append(w)\n",
    "        \n",
    "        # Check stopping criterion\n",
    "        grad_inf = np.linalg.norm(grad_risk(X, y, w), ord=np.inf)\n",
    "        if grad_inf <= tol:\n",
    "            break\n",
    "    \n",
    "    print(\"Stopping after {} iterations\".format(t))\n",
    "    print(\"Inf-norm of grad is {:.4g}\".format(grad_inf))\n",
    "    \n",
    "    return w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's run the algorithm on the 2D classification data we generated in Section 1 and visualise the result. Does the result look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_history_irls = fit_logistic(X_b, y, tol=1e-9)\n",
    "\n",
    "def plot_decision_boundary(X, y, w):\n",
    "    \"\"\"Plots the decision boundary of a logistic regression classifier defined \n",
    "    by weights `w`\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X_b[y==0,1], X_b[y==0,2], color='b', label=\"$y = 0$\")\n",
    "    ax.scatter(X_b[y==1,1], X_b[y==1,2], color='r', label=\"$y = 1$\")\n",
    "    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    ax.plot(list(xlim), [-w[0]/w[2] - w[1]/w[2] * x for x in xlim], ls = \"-\", color=\"k\")\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title(\"Decision boundary\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_b, y, w_history_irls[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also check the validity of our implementation by comparing with scikit-learn's implementation. Note that the scikit-learn implementation incorporates $L_2$ regularisation by default, so we need to switch it off by setting `penalty = 'none'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='none')\n",
    "clf.fit(X, y)\n",
    "w_sklearn = np.r_[clf.intercept_, clf.coef_.squeeze()]\n",
    "print(\"Weights according to IRLS: {}\".format(w_history_irls[-1]))\n",
    "print(\"Weights according to scikit-learn: {}\".format(w_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the path taken by the IRLS algorithm to reach the optimal solution.\n",
    "We plot the weight vectors at each iteration $\\mathbf{w}_0, \\mathbf{w}_1, \\ldots$ on top of contours of the empirical risk $\\hat{R}(\\mathbf{w})$. \n",
    "The darker the shade, the lower the empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_iterates(X, y, w_history):\n",
    "    \"\"\"Plots the path of iterates in weight space (excluding the bias)\"\"\"\n",
    "    w_history = np.array(w_history)\n",
    "    \n",
    "    # Compute axes limits\n",
    "    w12_max = w_history[:,1:].max()\n",
    "    w12_min = w_history[:,1:].min()\n",
    "    w12_ran = w12_max - w12_min\n",
    "    border = 0.1\n",
    "    \n",
    "    # Compute objective on grid\n",
    "    w12 = np.linspace(w12_min - border * w12_ran, w12_max + border * w12_ran, num=100)\n",
    "    w1v, w2v = np.meshgrid(w12, w12)\n",
    "    w12v = np.c_[w1v.ravel(), w2v.ravel()]\n",
    "    z = np.array([risk(X_b, y, np.r_[w_history[-1][0], w12]) for w12 in w12v])\n",
    "    z = z.reshape(w1v.shape)\n",
    "\n",
    "    plt.contourf(w1v, w2v, z, cmap='gist_gray')\n",
    "    plt.colorbar(label='Empirical risk')\n",
    "    plt.plot(w_history[:,1], w_history[:,2], c='c', ls='--')\n",
    "    plt.scatter(w_history[:,1], w_history[:,2], c='r', marker='.', label='Iterate')\n",
    "    plt.xlabel('$w_1$')\n",
    "    plt.ylabel('$w_2$')\n",
    "    plt.legend()\n",
    "    plt.title('Contour plot of iterates in weight space')\n",
    "    plt.show()\n",
    "\n",
    "plot_iterates(X_b, y, w_history_irls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Logistic regression via gradient descent\n",
    "\n",
    "Finally, let's compare the IRLS algorithm with gradient descent. \n",
    "To do this, we can reuse the `fit_logistic` function defined earlier. \n",
    "We just need to replace `update_weight_irls` with an analagous function for gradient descent.\n",
    "\n",
    "Recall that the weight update for gradient descent at iteration $t$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_t = \\mathbf{w}_{t - 1} - \\eta \\nabla_\\mathbf{w} \\hat{R}(\\mathbf{w}_{t - 1})\n",
    "$$\n",
    "\n",
    "where $\\eta > 0$ is a learning rate parameter.\n",
    "***\n",
    "**Exercise:** Complete the `update_weight_gd` function below which implements the weight update for gradient descent.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_weight_gd(X, y, w, **kwargs):\n",
    "    \"\"\"Performs a gradient descent weight update\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Feature matrix. The matrix must contain a constant column to \n",
    "        incorporate a non-zero bias.\n",
    "    \n",
    "    y : array of shape (n_samples,)\n",
    "        Response relative to X. Binary classes must be encoded as 0 and 1.\n",
    "    \n",
    "    w : array of shape (n_features,)\n",
    "        Current estimate of the weight vector.\n",
    "    \n",
    "    **kwargs : \n",
    "        Keyword arguments.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array of shape (n_features,)\n",
    "        Updated estimate of the weights vector.\n",
    "    \"\"\"\n",
    "    # Get learning rate from kwargs, defaulting to 1.0 if None\n",
    "    eta = kwargs.get(\"eta\", 1)\n",
    "    return ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's run our gradient descent implementation on the same classification data from before. \n",
    "What do you notice about the path taken by the gradient descent algorithm? \n",
    "How does it compare with IRLS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_history_gd = fit_logistic(X_b, y, update_weight=update_weight_gd, max_iter=1000, eta = 5, tol=1e-9)\n",
    "plot_decision_boundary(X_b, y, w_history_gd[-1])\n",
    "plot_iterates(X_b, y, w_history_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "## Bonus: Linearly separable case (optional)\n",
    "\n",
    "\n",
    "**Exercise:** What happens when you re-run the notebook with `RND_SEED = 90051`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}