{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 6\n",
    "## The Perceptron\n",
    "***\n",
    "In this worksheet, we'll implement the perceptron (a building block of neural networks) from scratch. \n",
    "Our key objectives are:\n",
    "\n",
    "* to review the steps involved in the perceptron training algorithm\n",
    "* to assess how the perceptron behaves in two distinct scenarios (separable vs. non-separable data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Synthetic data set\n",
    "We'll use the built-in `make_classification` function from `sklearn` to generate a synthetic binary classification data set.\n",
    "The main advantage of using synthetic data is that we have complete control over the distribution. \n",
    "This is useful for studying machine learning algorithms under specific conditions.\n",
    "In particular, we'll be varying the *degree of separability* between the two classes by adjusting the `class_sep` parameter below.\n",
    "To begin, we'll work with a data set that is almost linearly separable (with `class_sep = 1.75`).\n",
    "\n",
    "**Note:** In this worksheet we deviate from the standard `0`/`1` encoding for binary class labels used in `sklearn`. \n",
    "We use `-1` in place of `0` for the \"negative\" class to make the mathematical presentation of the perceptron algorithm easier to digest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_RESOLUTION = 128\n",
    "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
    "\n",
    "def create_toy_data(n_samples=150, class_sep=1.75):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=2, n_informative=2, \n",
    "                               n_redundant=0, n_clusters_per_class=1, flip_y=0,\n",
    "                               class_sep=class_sep, random_state=12)\n",
    "    y[y==0] = -1 # encode \"negative\" class using -1 rather than 0\n",
    "    plt.plot(X[y==-1,0], X[y==-1,1], \".\", label=\"$y = -1$\")\n",
    "    plt.plot(X[y==1,0], X[y==1,1], \".\", label=\"$y = 1$\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")\n",
    "    plt.show()\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X, y = create_toy_data(class_sep = 1.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is the perceptron a suitable classifier for this data set?\n",
    "\n",
    "  \n",
    "In preparation for training and evaluating a perceptron on this data, we randomly partition the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definition of the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lectures that a perceptron is a linear binary classifier which maps an input vector $\\mathbf{x} \\in \\mathbb{R}^D$ to a binary output $y \\in \\{-1,1\\}$ given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}; \\mathbf{w}, b) &= \\begin{cases}\n",
    "    1 & \\mathrm{if} \\ s(\\mathbf{x}; \\mathbf{w}, b) \\geq 0, \\\\\n",
    "    -1 & \\mathrm{otherwise},\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $s(\\mathbf{x}; \\mathbf{w}, b) = \\mathbf{w} \\cdot \\mathbf{x} + b$. \n",
    "Here $\\mathbf{w} \\in \\mathbb{R}^D$ is a vector of weights (one for each feature) and $b$ is the bias term. Note that for this worksheet we are including an explicit bias, rather than incorporating it as a constant column in $\\mathbf{X}$.\n",
    "\n",
    "Let start by implementing the weighted sum function $s(\\mathbf{x}; \\mathbf{w}, b)$ below. This is the 'inner product' $\\langle\\mathbf{w}, \\mathbf{x}\\rangle = \\sum_i w_i x_i$, which captures the geometrical alignment of the weight and instance vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X, w, b):\n",
    "    \"\"\"\n",
    "    Returns an array containing the weighted sum s(x) for each instance x in the feature matrix\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        weights vector\n",
    "    b : float\n",
    "        bias term\n",
    "    \"\"\"\n",
    "    return ... # fill in\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Returns an array containing the predicted binary labels (-1/1) for each instance in the feature matrix\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        weights vector\n",
    "    b : float\n",
    "        bias term\n",
    "    \"\"\"\n",
    "    return ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perceptron training algorithm\n",
    "\n",
    "We will now implement the Perceptron training algorithm first proposed by Rosenblatt in 1957. \n",
    "\n",
    "This is an online algorithm in the sense that it examples one point $\\mathbf{x}_t$ at a time. At each stage, the algorithm maintains a weight vector $\\mathbf{w}_t$ which defines the learned (hyper)plane separating the two classes. For convenience one typically starts with $\\mathbf{w}_0 = 0$. The label of the point $\\mathbf{x}_t$ is predicted using the sign of the dot product: $\\hat{y}_t = \\mathrm{sign}(\\mathbf{w}_t \\cdot \\mathbf{x}_t)$. If $\\mathbf{x}_t$ is misclassified, $y_t \\mathbf{x} \\cdot \\mathbf{w}_t$ is negative and we make a correction to the weight vector $\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\eta y_t \\mathbf{x}_t$ for some learning rate $\\eta >0$. To motivate this, consider the dot product of the weight vector after the update with the misclassified instance: \n",
    "\n",
    "\\begin{align}\n",
    "y_t (\\mathbf{w}_t + \\eta y_t \\mathbf{x}_t) \\cdot \\mathbf{x}_t &= y_t \\mathbf{w}_t \\cdot \\mathbf{x}_t + \\eta ||\\mathbf{x}_t ||^2\\\\\n",
    "&\\geq y_t \\mathbf{w}_t \\cdot \\mathbf{x}_t\n",
    "\\end{align}\n",
    "\n",
    "So we reduce the severity of the error by an additive correction $\\eta || \\mathbf{x}_t ||^2$. We repeat this procedure until convergence or termination after a set limit of iterations. Pseudocode of the algorithm is shown below (taken from _Foundations of Machine Learning, Mohri, 2019_).\n",
    "\n",
    "<img src=\"https://i.imgur.com/N6NZrAC.png\" alt=\"Perceptron alg.\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** After iterating through all of the training instances $t=[1, \\ldots, T]$, we have completed one *epoch*. The above pseudocode runs for one epoch. Typically, multiple epochs are required to get close to the optimal solution.\n",
    "\n",
    "The above training procedure is equivalent to performing sequential gradient descent on the following objective function:\n",
    "\n",
    "\\begin{align}\n",
    "    F(\\mathbf{w}) &= \\frac{1}{T} \\sum_{t=1}^T \\max\\left(0, -y_t (\\mathbf{w} \\cdot \\mathbf{x}_t)\\right) \\\\\n",
    "    \\mathbf{w}_{t+1} &\\leftarrow \\mathbf{w}_t - \\eta \\nabla_{\\mathbf{w}} F(\\mathbf{w}) \n",
    "\\end{align}\n",
    "\n",
    "To motivate the form of $F(\\mathbf{w})$, note that if $\\mathbf{x}_t$ is misclassified, $-y_t \\mathbf{w}_t \\cdot \\mathbf{x}_t > 0$. In this case the gradient is $\\nabla_{\\mathbf{w}} F(\\mathbf{w}) = -y_t \\mathbf{x}_t$, and zero if the instance is correctly classified. Omitting technical details about the case where $\\mathbf{w}_t \\cdot \\mathbf{x}_t = 0$ (we can resolve the non-differentiability by working with _subgradients_), the stochastic online update becomes:\n",
    "\n",
    "$$ \\mathbf{w}_{t+1} \\leftarrow\n",
    "  \\begin{cases}\n",
    "    \\mathbf{w}_t + \\eta y_t \\mathbf{x}_t, & \\text{if } y_t\\left(\\mathbf{x}_t \\cdot \\mathbf{w}_t\\right) \\leq 0; \\\\\n",
    "    \\mathbf{w}_t, & \\text{if } y_t\\left(\\mathbf{x}_t \\cdot \\mathbf{w}_t\\right) > 0 \\\\\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Exercise:** Find weights of a perceptron of the following dataset, with learning rate $\\eta =0.1$\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \\begin{pmatrix} 1&1&1 \\\\ 1&1&2 \\\\ 1&0&0 \\\\ 1&-1 & 0\\end{pmatrix},  \\mathbf{y} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write a function called `train` which implements sequential gradient descent. The function should implement the pseudocode shown above, repeated for `n_epochs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, w, b, n_epochs=10, eta=0.1):\n",
    "    \"\"\"\n",
    "    Returns updated weight vector w and bias term b for\n",
    "    the Perceptron algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    y : numpy array, shape: (n_instances,)\n",
    "        target class labels relative to X\n",
    "    n_epochs : int\n",
    "        number of epochs (full sweeps through X)\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        initial guess for weights vector w_0\n",
    "    b : float\n",
    "        initial guess for bias term b_0\n",
    "    eta : positive float\n",
    "        step size (default: 0.1)\n",
    "    \"\"\"\n",
    "    errors = 0\n",
    "    for t in range(n_epochs):\n",
    "        # loop over individual elements in X\n",
    "        for i in range(X.shape[0]):\n",
    "            yhat = ... # fill in\n",
    "            if yhat != y[i]:\n",
    "                errors += 1\n",
    "                # Update if misclassified, else do nothing\n",
    "                w += ... # fill in\n",
    "                b += ... # fill in\n",
    "                \n",
    "    print('Number of errors {} / {} iterations'.format(errors, n_epochs * X.shape[0]))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running it for 5 epochs.\n",
    "You should get the following result for the weights and bias term:\n",
    "`w = [ 0.78555827 -0.11419669]; b = 0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise weights and bias to zero\n",
    "w = np.zeros(X.shape[1]); b = 0.0\n",
    "\n",
    "w, b = train(X_train, y_train, w, b, n_epochs=5)\n",
    "print(\"w = {}; b = {}\".format(w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the perceptron, let's see how it performs.\n",
    "\n",
    "Below we plot the data (training and test sets) along with the decision boundary (which is defined by $\\{\\mathbf{x}: \\hat{\\mathbf{w}} \\cdot \\mathbf{x}= 0$\\})\n",
    "**Note:** It's not necessary to understand how the `plot_results` function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(X_train, y_train, X_test, y_test, score_fn, threshold = 0):\n",
    "    # Plot training set\n",
    "    plt.plot(X_train[y_train==-1,0], X_train[y_train==-1,1], \".\", label=r\"$y=-1$, train\")\n",
    "    plt.plot(X_train[y_train==1,0], X_train[y_train==1,1], \".\", label=r\"$y=1$, train\")\n",
    "    plt.gca().set_prop_cycle(None) # reset colour cycle\n",
    "\n",
    "    # Plot test set\n",
    "    plt.plot(X_test[y_test==-1,0], X_test[y_test==-1,1], \"x\", label=r\"$y=-1$, test\")\n",
    "    plt.plot(X_test[y_test==1,0], X_test[y_test==1,1], \"x\", label=r\"$y=1$, test\")\n",
    "\n",
    "    # Compute axes limits\n",
    "    border = 1\n",
    "    x0_lower = X[:,0].min() - border\n",
    "    x0_upper = X[:,0].max() + border\n",
    "    x1_lower = X[:,1].min() - border\n",
    "    x1_upper = X[:,1].max() + border\n",
    "\n",
    "    # Generate grid over feature space\n",
    "    resolution = 0.01\n",
    "    x0, x1 = np.mgrid[x0_lower:x0_upper:resolution, x1_lower:x1_upper:resolution]\n",
    "    grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "    s = score_fn(grid).reshape(x0.shape)\n",
    "\n",
    "    # Plot decision boundary (where s(x) == 0)\n",
    "    plt.contour(x0, x1, s, levels=[0], cmap=\"Greys\", vmin=-0.2, vmax=0.2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_results(X_train, y_train, X_test, y_test, lambda X: weighted_sum(X, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How well does the decision boundary separate the points in the two classes? How does it perform if you reduce the `class_sep` parameter in the `create_toy_dataset` function above?\n",
    "\n",
    "\n",
    "To evaluate the perceptron quantitatively, we'll use the error rate (proportion of misclassified instances).\n",
    "The error rate is a reasonable evaluation measure to use for this data since the classes are well balanced.\n",
    "\n",
    "Complete the `evaluate` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Returns the proportion of misclassified instances (error rate)\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    Y : numpy array, shape: (n_instances,)\n",
    "        target class labels relative to X\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        weights vector\n",
    "    b : float\n",
    "        bias term\n",
    "    \"\"\"\n",
    "    return ... # fill in\n",
    "\n",
    "print(evaluate(X_train, y_train, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block above computes the error rate on the training data, which is not a good idea in general. (Why?)\n",
    "\n",
    "Compute the error rate on the test set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(X_test, y_test, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does this compare to the error on the training set? Is it what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Repeat with class overlap\n",
    "By now you've probably concluded that the perceptron performs well on this data (`class_sep=1.75`), which is to be expected as it's roughly linearly separable. \n",
    "However, we know (from lectures) that the perceptron can fail on non-linearly separable data.\n",
    "To test this scenario, re-generate the synthetic data set with `class_sep=0.5` and repeat Sections 2–4.\n",
    "\n",
    "**Question:** What do you find? Pay particular attention to the error vs. training epochs (you may need to add a `print` command in the inner training loop). Do you notice anything unusual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reimplement Preceptron in PyTorch\n",
    "\n",
    "In this last section we will reimplement the Perceptron algorithm in PyTorch.\n",
    "\n",
    "Armed with this shiny deep learning API, let's rewrite the perceptron training algorithm. We have two options: just using the standard perceptron update formula, or providing torch with the loss function and letting it compute the gradient automatically. We'll go with the latter, a more general solution, which is similar to how you might train a logistic regerssion classification model, regression, counting likelihood etc. In these cases you would change the `loss =` line to reflect the different loss function, e.g., logistic cross entropy, squared error (Gaussian), Poisson loss for the three cases, respectively.\n",
    "\n",
    "Fill in the code below, and use the [PyTorch API](https://pytorch.org/docs/stable/torch.html#) to understand the operations. Some step are a little cryptic, and relate to enumerating over the training data, setting the shape of the tensors (e.g., `squeeze` which removes redundant dimensions of size 1, `requires_grad_` which flags a parameter for which `backward` will compute the gradient.) Take a look at the [SGD class, and other optimisers](https://pytorch.org/docs/stable/optim.html) and their various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron_SGD_pytorch(X, y, n_epochs=30, eta=0.1):\n",
    "    \n",
    "    # Create iterable dataset in Torch format\n",
    "    \n",
    "    X = torch.cat([torch.ones(X.shape[0],1), X], dim=1)\n",
    "    train_ds = torch.utils.data.TensorDataset(X, y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1)\n",
    "    \n",
    "    # Create trainable weight vector, tell Torch to track operations on w\n",
    "    w = torch.zeros(X.shape[1]).double().requires_grad_()\n",
    "    \n",
    "    # Setup the optimizer. This implements the basic gradient descent update\n",
    "    optimizer = ... # fill in\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        for i, (xi,yi) in enumerate(train_loader):\n",
    "            xi = torch.squeeze(xi)\n",
    "\n",
    "            #Compute loss F(w)\n",
    "            loss = torch.max(torch.Tensor([0]), -yi * (torch.dot(w,xi)))\n",
    "            \n",
    "            ... # fill in \n",
    "            \n",
    "        \n",
    "    w = w.detach()  # Tell Torch to stop tracking operations for autograd\n",
    "    print('Trained weights:', w)\n",
    "    return w.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the function above to verify it reaches a similar solution to the `numpy` code above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train_perceptron_SGD_pytorch(torch.from_numpy(X_train), torch.from_numpy(y_train),n_epochs = 5)\n",
    "\n",
    "plot_results(X_train, y_train, X_test, y_test, score_fn=lambda x: np.dot(x,w[1:])+w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks (optional)\n",
    "\n",
    "### Comparison with logistic regression\n",
    "We've seen that the perceptron is not robust to binary classification problems with overlapping classes. \n",
    "But how does logistic regression fare in this case?\n",
    "\n",
    "Adapt the code block above to fit a logistic regression model. To do so you will need to change the loss function to the cross entropy loss from the logistic regression model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
