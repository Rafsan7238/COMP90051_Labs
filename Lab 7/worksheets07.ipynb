{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COMP90051 Workshop 7\n",
    "## Model Design in PyTorch / Convolutional Neural Networks\n",
    "\n",
    "During the previous two weeks, we worked through the fundamentals of [PyTorch](https://pytorch.org/) and saw the utility of Automatic on-the-fly differentiation ([Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)) for gradient-based optimization. In this workshop we will we'll implement a convolutional neural network (CNN) in Pytorch to handle the other computer vision tasks.\n",
    "\n",
    "## Image Classification on CIFAR-10\n",
    "Here we will tackle a supervised learning problem on a canonical image dataset, `CIFAR-10`. This consists of $32 \\times 32$ 3-channel color images arriving in 10 classes of various animals and vehicles. Lets take a look at some randomly sampled images below, using a handy convenience function from Torch to download and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "import numpy as np\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "cifar10_transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar10_transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar10_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "label_class_map = dict(zip(range(10), classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FIGURE_RESOLUTION = 256\n",
    "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(str(title))\n",
    "    plt.show()\n",
    "\n",
    "images, labels = iter(train_loader).next()\n",
    "images = images[:8]\n",
    "labels = labels[:8]\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print([label_class_map[c.item()] for c in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model design with `torch.nn.Module`\n",
    "\n",
    "We'll want to compare a couple of different architectures when attacking this problem. Iterating over different models in the ad-hoc manner we saw in previous workshops can quickly get messy. Model design typically (but not always!) decomposes into an _initialization phase_, where the model parameters are defined, and the _forward phase_, where the input tensors $\\mathbf{x}$ pass through the Directed Acyclic Graph of operations to produce the model output, the logits $\\Phi(\\mathbf{x})$. The canonical method of model design in PyTorch reflects this decomposition. We start with inheriting from `torch.nn.Module`, which allows us to access common NN-specific functionality, then:\n",
    "\n",
    "* Implement the constructor `__init__(self, ...)`. Here we define all network parameters.\n",
    "* Override the forward method `forward(self, x)`. This accepts the input tensor `x` and returns our desired model output.\n",
    "\n",
    "Provided your operations are autograd-compliant, the backward pass is implemented automatically as PyTorch walks the computational graph backward. Imagine if you had to manually compute your own parameter gradients - what a time-saver! We'll look at multiple examples of this today, starting with the logistic regression model we investigated last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        \n",
    "        # Register weight matrix and bias term as model parameters - automatically tracks operations for gradient computation.\n",
    "        self.W = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty([n_features, n_classes])))  # Weights\n",
    "        self.b = torch.nn.Parameter(torch.zeros([n_classes]))  # Biases\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for logistic regression.\n",
    "        Input: Tensor x of shape [N,C,H,W]\n",
    "        Output: Logits W @ x + b\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.view(batch_size, -1)  # Flatten image into vector, retaining batch dimension\n",
    "        out = torch.matmul(x,self.W) + self.b  # Compute scores\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The parameters added as class attributes are now associated with your module, access them using `Module.parameters()`. Also note the `(...)` operator is redefined to call the `forward` method. You may want to check out the documentation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features, n_classes = 32*32*3, 10  # Here we flatten the 3D image into a 1D vector\n",
    "logistic_regression_model = LogisticRegressionModel(n_features, n_classes)\n",
    "\n",
    "for p in logistic_regression_model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll write a convenient `train` and `test` function that allows us to seamlessly substitute different models - this is essential for fast iteration during development in The Real World. The basic structure is identical to what you encountered last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0.\n",
    "    test_preds, test_labels = list(), list()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, labels = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)  # Compute scores\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_loss += criterion(input=logits, target=labels).item()\n",
    "            test_preds.append(predictions)\n",
    "            test_labels.append(labels)\n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "\n",
    "    test_accuracy = torch.eq(test_preds, test_labels).float().mean().item()\n",
    "\n",
    "    print('[TEST] Mean loss {:.4f} | Accuracy {:.4f}'.format(test_loss/len(test_loader), test_accuracy))\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, n_epochs=10):\n",
    "    \"\"\"\n",
    "    Generic training loop for supervised multiclass learning\n",
    "    \"\"\"\n",
    "    LOG_INTERVAL = 250\n",
    "    running_loss, running_accuracy = list(), list()\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):  # Loop over training dataset `n_epochs` times\n",
    "\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
    "\n",
    "            x, labels = data\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, labels).float()).item()\n",
    "\n",
    "            loss = criterion(input=logits, target=labels)\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "\n",
    "            # ============================================================================\n",
    "            # You can safely ignore the boilerplate code below - just reports metrics over\n",
    "            # training and test sets\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_accuracy.append(train_acc)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:  # Log training stats\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, train_acc, deltaT))\n",
    "\n",
    "        print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "\n",
    "        test(model, criterion, test_loader)\n",
    "        \n",
    "    return running_loss, running_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the model parameters into our selected optimizer and we're good to go. We'll include a `momentum` term in the standard SGD update rule to accelerate convergence. Intiutively, this helps the optimizer ignore parameter updates in suboptimal directions, possibly due to noise in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(logistic_regression_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "lr_loss, lr_acc = train(logistic_regression_model, train_loader, test_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should be getting a test accuracy just below 40% at the conclusion of training (you may want to try playing around with the optimizer later on to see if you can get better results). While this is significantly better than random chance, this is still a bit of a silly approach because we ignore all spatial structure of the input images. We are also _a priori_ treating all pixels in the image identically, irrespective of their separation/proximity to other pixels. Lets now consider using convolutional networks for the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convolutional networks exploit the \"translation invariance\" property seen in natural images. A representation useful at a certain spatial location is typically useful across the whole image. The convolution operation applies the same linear transformation in different local regions across the image, allowing the model to learn local features depending only on small regions of the image. For a more thorough discussion one may refer to Bishop, Section 5.5.6. We will implement a small convolutional network described below:\n",
    "\n",
    "1. *Convolutional Layer #1* | 8 5Ã—5 filters with a stride of 1, ReLU activation function.\n",
    "2. *Max Pooling #1*         | Kernel size 2 with a stride of 2.\n",
    "3. *Convolutional Layer #2* | 16 5Ã—5 filters with a stride of 1, ReLU activation function.\n",
    "4. *Max Pooling #2*         | Kernel size 2 with a stride of 2.\n",
    "5. *Fully Connected Layer #1* | 400 input units (flattened convolutional output), 256 output units.\n",
    "5. *Fully Connected Layer #2* | 256 input units, 10 output units - yields logits for classification.\n",
    "\n",
    "In the interests of training time, we keep the number of parameters low by restricting the size of the output channels. Increasing this would most likely increase the classification performance of our network at the expense of additional compute. If you have time you may want to experiment with the architecture. Some questions you may want to consider - is a larger number of parameters always better? How should we adjust the learning rate if we increase the number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OUT_C1 = 8\n",
    "OUT_C2 = 16\n",
    "DENSE_UNITS = 256\n",
    "\n",
    "class BasicConvNet(nn.Module):\n",
    "    def __init__(self, out_c1, out_c2, dense_units, n_classes=10):\n",
    "        super(BasicConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=out_c1, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_c1, out_channels=out_c2, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, dense_units)\n",
    "        self.logits = nn.Linear(dense_units, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = self.logits(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "conv2D_model = BasicConvNet(OUT_C1, OUT_C2, DENSE_UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The architecture for the default setting is shown below (generated using http://alexlenail.me/NN-SVG/LeNet.html).\n",
    "\n",
    "<img src=\"https://i.imgur.com/vNd3Lkg.png\" alt=\"CNN arch\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "**Question:** Calculate the number of parameters in the logistic regression model and the above convnet. The diagram above may be a useful guide. You may also want to look at the `model.parameters()` method for each `model`.\n",
    "\n",
    "We see that the convolutional parameters are relatively lightweight and most of the parameters are attributed to the fully connected layers.\n",
    "\n",
    "As before, we have to tell the chosen optimizer which parameters to learn, we'll use the same optimizer for the sake of a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(conv2D_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "conv_loss, conv_acc = train(conv2D_model, train_loader, test_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should be seeing a ~25 % increase in test performance just by swapping out the logistic regression architecture for our modest 2D convolutional net! Here the exact convolutional architecture is of secondary importance to our modular approach to model design. Plase ask your tutor if anything is unclear on this front. We'll plot the loss curves below and move on. The downward slope of the loss curve at termination suggests we have not reached full convergence yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter  # Smooth spiky curves\n",
    "smooth_lr_loss, smooth_conv_loss = savgol_filter(lr_loss, 21, 3), savgol_filter(conv_loss, 21, 3)\n",
    "smooth_lr_acc, smooth_conv_acc = savgol_filter(lr_acc, 21, 3), savgol_filter(conv_acc, 21, 3)\n",
    "plt.rcParams['figure.dpi'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(smooth_lr_loss, label='Logistic Regression')\n",
    "plt.plot(smooth_conv_loss, label='Conv Net')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss (Train)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(smooth_lr_acc, label='Logistic Regression')\n",
    "plt.plot(smooth_conv_acc, label='Conv Net')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (Train)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Your turn ðŸ¤”: Convolutional Autoencoders\n",
    "\n",
    "## Autoencoders\n",
    "Recall from lectures that an autoencoder is used to learn an 'efficient' or lower dimensional coding of the input $\\mathbf{x} \\in \\mathbb{R}^n$ to some latent code $\\mathbf{z} \\in \\mathbb{R}^d$. The intuitive idea is that we wish to recover the high-dimensional, potentially sparse data signal represented by $\\mathbf{x}$ from some low-dimensional projection $\\mathbf{z}$. Natural images are a good example of data which is potentially very high-dimensional (for an $m \\times n$ color image, $\\mathbf{x} \\in \\mathbb{R}^{3mn}$ naively, but we would only expect valid inputs to lie in some relatively small subspace of the original space, which can be spanned using fewer 'meaningful' dimensions. By enforcing a lower-dimensional projection, we would like our representation to discard redundant dimensions while retaining dimensions which correspond to intrinsic aspects of the input space. Autoencoding-based models have many applications such as image/speech synthesis, super-resolution and compressed-sensing. \n",
    "\n",
    "The autoencoder is trained in an unsupervised manner. It is composed of two components:\n",
    "* The _encoder_ $f$ from the original space to the latent space, $f(\\mathbf{x}) = \\mathbf{z}$\n",
    "* The _decoder_ $g$ from the latent space to the original space, $g(\\mathbf{z}) = \\mathbf{\\hat{x}}$\n",
    "\n",
    "The autoencoder parameters are learnt such that $g \\circ f$ is close to the identity when averaged over the training set. As the latent space is typically much lower dimensional than the original space, the encoder needs to learn a compact representation of the original data that contains sufficient information for the decoder to reconstruct.\n",
    "\n",
    "\n",
    "The simplest autoencoder model occurs when both the encoder and decoder are linear functions. It is well known (Bourlard and Kamp 1988) that for a linear autoencoder with encoder and decoder functions represented by matrices:\n",
    "* $Y \\in \\mathbb{R}^{d \\times n}$\n",
    "* $Y' \\in \\mathbb{R}^{n \\times d}$\n",
    "\n",
    "respectively, then the quadratic error objective \n",
    "\n",
    "$$ \\min_{Y, Y'} \\sum_k \\Vert \\mathbf{x}_k - YY'\\mathbf{x}_k \\Vert^2 $$\n",
    "is minimized by the PCA decomposition (as you'll see in week 9).\n",
    "\n",
    "For more general mappings we minimize an empirical estimate of the expected quadratic loss:\n",
    "\n",
    "$$ \\min_{f,g} \\sum_k \\Vert \\mathbf{x}_k - g \\circ f(\\mathbf{x}_k) \\Vert^2 $$\n",
    "\n",
    "We will use a fully convolutional network for the encoder and a decoder composed of the reciprocal transposed convolution layers (essentially the inverse of the convolution operator - needed to upsample the compressed image).\n",
    "\n",
    "<img src=\"https://i.imgur.com/Q69VB3H.png\" alt=\"AE\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Your task is to define the convolutional encoder we'll be using through `torch.nn.Module`. We'll define the decoder for you, which you can use as a template to build the encoder. Note that it is conventional for the decoder/encoder to mirror one another in terms of the nonlinearities, kernel sizes and strides used at each stage. The basic structure of our architecture is very simple, and you should implement the encoder structure.\n",
    "\n",
    "* Encoder \n",
    "    1. *Convolutional Layer #1* | 8 4Ã—4 filters with a stride of 2, padding 1, ReLU activation function.\n",
    "    2. *Convolutional Layer #2* | 16 4Ã—4 filters with a stride of 2, padding 1, ReLU activation function.\n",
    "    3. *Convolutional Layer #3* | 32 4Ã—4 filters with a stride of 2, padding 1 - the output of this layer is the latent code.\n",
    "\n",
    "\n",
    "* Decoder (mirror encoder structure)\n",
    "    4. *Transposed Convolution #1* | 32 4Ã—4 filters with a stride of 2, padding 1, ReLU activation function.\n",
    "    5. *Transposed Convolution #2* | 16 4Ã—4 filters with a stride of 2, padding 1, ReLU activation function.\n",
    "    6. *Transposed Convolution #3* | 8 4Ã—4 filters with a stride of 2, padding 1, sigmoid activation function.\n",
    "\n",
    "We apply a sigmoid activation function at the last layer to keep the output within the range $[0,1]$. \n",
    "\n",
    "You may find the `torch.nn.Module` reference to be useful here: https://pytorch.org/docs/stable/nn.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OUT_C1 = 8\n",
    "OUT_C2 = 16\n",
    "OUT_C3 = 32\n",
    "OUT_C4 = 32\n",
    "EMBEDDING_DIMENSION = 32\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, out_c1, out_c2, out_c3, out_c4, embedding_dim):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = ... # fill in\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ... # fill in\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, out_c1, out_c2, out_c3, out_c4, embedding_dim):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=embedding_dim, out_channels=out_c2, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=out_c2, out_channels=out_c1, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=out_c1, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        out = torch.sigmoid(self.deconv3(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that we could have defined a single `Autoencoder(nn.Module)` subclass with `encode(...)` and `decode(...)` defined as distinct methods as well. Since we're considering an unsupervised setup the training loop is slightly different. Training is conducted using the above quadratic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(encoder, decoder, train_loader, test_loader, optimizer, n_epochs=10, cuda=False):\n",
    "    \"\"\"\n",
    "    Generic training loop for supervised multiclass learning\n",
    "    \"\"\"\n",
    "    LOG_INTERVAL = 250\n",
    "    running_loss = list()\n",
    "    start_time = time.time()\n",
    "    # criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "    criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    for epoch in range(n_epochs):  # Loop over training dataset `n_epochs` times\n",
    "\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
    "            \n",
    "            x, _ = data\n",
    "            batch_size = x.shape[0]\n",
    "            \n",
    "            if cuda is True:  # Send to GPU\n",
    "                x = x.to(device)\n",
    "\n",
    "            code = encoder(x)\n",
    "            reconstructed_x = decoder(code)\n",
    "\n",
    "            loss = criterion(input=reconstructed_x, target=x) / batch_size\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "\n",
    "            # ============================================================================\n",
    "            # You can safely ignore the boilerplate code below - just reports metrics over\n",
    "            # training and test sets\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:  # Log training stats\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Time {:.2f} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, deltaT))\n",
    "\n",
    "        print('Epoch complete! Mean training loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "\n",
    "        test_loss = 0.\n",
    "        \n",
    "        for i, data in enumerate(test_loader):\n",
    "            x, _ = data\n",
    "\n",
    "            if cuda is True:  # Send to GPU\n",
    "                x = x.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                code = encoder(x)\n",
    "                reconstructed_x = decoder(code)\n",
    "\n",
    "                test_loss += criterion(input=reconstructed_x, target=x).item() / batch_size\n",
    "\n",
    "        print('[TEST] Mean loss {:.4f}'.format(test_loss/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we initialize the encoder and decoder modules. We pass the parameter lists of both modules to our optimizer. Since `model.parameters()` returns a generator we'll `chain` both generators together. We'll use the same SGD w/ momentum optimizer, except add Nesterov momentum to stabilize training and accelerate convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "use_cuda = True  # Set this to true if using a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = ConvEncoder(OUT_C1, OUT_C2, OUT_C3, OUT_C4, EMBEDDING_DIMENSION)\n",
    "decoder = ConvDecoder(OUT_C1, OUT_C2, OUT_C3, OUT_C4, EMBEDDING_DIMENSION)\n",
    "\n",
    "if use_cuda is True:\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# optimizer = torch.optim.SGD(chain(encoder.parameters(), decoder.parameters()), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = torch.optim.Adam(chain(encoder.parameters(), decoder.parameters()), lr=1e-3)\n",
    "\n",
    "train_autoencoder(encoder, decoder, train_loader, test_loader, optimizer, cuda=use_cuda, n_epochs=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below we'll grab 8 random images from the test set and plot the images along with their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 256\n",
    "test_im_generator = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = test_im_generator.next()\n",
    "images = images[:8]\n",
    "labels = labels[:8]\n",
    "\n",
    "with torch.no_grad():\n",
    "    code = encoder(images.to(device))\n",
    "    reconstructed_images = decoder(code)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images), title='Original')\n",
    "imshow(torchvision.utils.make_grid(reconstructed_images), title='Reconstructed')\n",
    "print([label_class_map[c.item()] for c in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Not bad! Looks like we were able to recover most of the salient features from our lower dimensional projection. The reconstructed images are somewhat recognizable as their original classes. You may want to try adjusting the architecture / adding layers, or changing the latent code dimension to see the effect on the reconstruction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extension: Interpolations\n",
    "A popular application of autoencoder models is the generation of a semantically meaningful latent space. To test if our autoencoder has learnt a well-structured latent space, one can decode a convex combination of the latent codes for datapoints, $\\mathbf{x}$ and $\\mathbf{x}'$. If interpolating between two points in latent space produces a meaningful image in data space that is not a pixelwise admixture of $\\mathbf{x}$ and $\\mathbf{x'}$, this suggests that nearby points in latent space are semanatically similar - i.e. the different classes are clustered together in latent space. This property of an 'interpretable' latent space may be a useful representation for downstream tasks.\n",
    "\n",
    "Concretely, we achieve this by interpolating the samples $x$ and $x'$ along a line in latent space.\n",
    "\n",
    "$$ I(x, x'; \\alpha) = g\\left((1-\\alpha) f\\left(x\\right) + \\alpha f(x')\\right)$$\n",
    "\n",
    "<img src=\"https://i.imgur.com/QgCkUTa.png\" alt=\"AE_int\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Choose images to interpolate between, add batch dimension\n",
    "x_i, x_f = torch.unsqueeze(images[0], dim=0), torch.unsqueeze(images[3], dim=0)\n",
    "encoder.to('cpu');\n",
    "decoder.to('cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "interpolations = list()\n",
    "latent_interpolations = list()\n",
    "line = torch.linspace(0,1,steps=8)\n",
    "\n",
    "# Get latent representations of images\n",
    "with torch.no_grad():\n",
    "    code_i = encoder(x_i)\n",
    "    code_f = encoder(x_f)\n",
    "\n",
    "for alpha in line:\n",
    "    \n",
    "    # Interpolate along line in data space\n",
    "    x_interpolated = alpha * x_f + (1-alpha) * x_i\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Interpolate along line in latent space\n",
    "        reconstruction = decoder(alpha * code_f + (1-alpha) * code_i)\n",
    "    \n",
    "    interpolations.append(x_interpolated)\n",
    "    latent_interpolations.append(reconstruction)\n",
    "\n",
    "interpolations = torch.cat(interpolations)\n",
    "latent_interpolations = torch.cat(latent_interpolations)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(interpolations), title='Interpolation in original space')\n",
    "imshow(torchvision.utils.make_grid(latent_interpolations), title='Interpolation in latent space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should find that interpolations in the latent space tends to be more structured than interpolation in the original pixel space - in the sense that it is more than just pixelwise interpolation. Each reconstructed image should resemble one class instead of an admixture of two classes. Conventional autoencoders aren't strictly the best task for this because they don't enforce any structure on the latent space. The [variational autoencoder](http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/) explicitly enforces some sort of user-specified structure on the latent space and typically leads to superior latent space representations. It is one of the most prominent marriages of deep learning with traditional Bayesian methods. But this is out of the scope of the subject and we're out of time - that's all for this week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}