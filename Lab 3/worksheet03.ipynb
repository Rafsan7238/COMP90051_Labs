{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COMP90051 Workshop 3\n",
    "## Linear regression\n",
    "\n",
    "***\n",
    "\n",
    "In this worksheet, we'll explore ordinary least squares (OLS) regression: both simple linear regression and basis function regression. Our key objectives are:\n",
    "\n",
    "* to implement OLS regression using numerical linear algebra functions from the `numpy` library\n",
    "* to practice using the scikit-learn interface for linear regression\n",
    "* to implement basis function regression as an extension of OLS linear regression\n",
    "\n",
    "Firstly, we'll import the relevant libraries (`numpy`, `matplotlib`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. The Boston Housing dataset\n",
    "\n",
    "Throughout this worksheet, we'll use the _Boston Housing dataset_ as an example. \n",
    "It contains data about towns in the Boston area, which can be used to predict median house values. \n",
    "There are 506 observations in total, each of which is described by 13 features, such as _per capita crime rate_, _percentage of population with a lower socio-economic status_, etc. \n",
    "You can read more about the features [here](http://lib.stat.cmu.edu/datasets/boston).\n",
    "\n",
    "Let's begin by loading the data and converting to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  \n0     15.3  396.90   4.98  \n1     17.8  396.90   9.14  \n2     17.8  392.83   4.03  \n3     18.7  394.63   2.94  \n4     18.7  396.90   5.33  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "ds = pd.DataFrame(np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]), columns=columns)\n",
    "y = pd.Series(raw_df.values[1::2, 2], name=\"MEDV\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To keep things simple, we'll work with a single feature called `LSTAT` for now. \n",
    "It corresponds to the percentage of the population in the town classified as 'lower status' by the US Census service in 1978. \n",
    "Note that the response variable (the median house value in the town) is denoted `MEDV`.\n",
    "Plotting the  `MEDV` vs. `LSTAT` we see that a linear model appears plausible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = ['LSTAT']\n",
    "\n",
    "for f in features:\n",
    "    plt.figure()\n",
    "    plt.scatter(ds[f], y, marker='.')\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel(r'Median House Value ($\\times 10^3$ USD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "**Question:** Have we made any mistakes in our analysis so far?\n",
    "***\n",
    "\n",
    "Let's now randomly split the data into training and test sets.\n",
    "This is necessary, so we can assess the generalisation error of our model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(ds, y, test_size=0.2, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "# select subset of the features\n",
    "X_train_s = X_train[features].values\n",
    "X_test_s = X_test[features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Linear algebra solution\n",
    "\n",
    "Let's now fit a linear regression model to the single-featured Boston Housing data. \n",
    "In order to appreciate the lower-level details, we'll start by fitting the model using numerical linear algebra functions. \n",
    "The same approach is used under the hood in machine learning libraries, such as scikit-learn.\n",
    "\n",
    "***\n",
    "**Exercise:** In lectures, we derived an analytic expression for the optimal weights $\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$. Attempt the derivation yourself using the following matrix calculus identities: \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\nabla_{\\mathbf{w}} \\mathbf{w}^\\top \\mathbf{x} = \\nabla_{\\mathbf{w}} \\mathbf{x}^\\top \\mathbf{w} = \\mathbf{x}^\\top \\\\\n",
    "    \\nabla_{\\mathbf{w}} \\mathbf{A}\\mathbf{w} = \\mathbf{A} \\\\\n",
    "    \\nabla_{\\mathbf{w}} \\mathbf{w}^\\top \\mathbf{A}\\mathbf{w} = \\mathbf{w}^\\top \\left(\\mathbf{A}^\\top + \\mathbf{A}\\right)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "where vector $\\mathbf{x}$ and matrix $\\mathbf{A}$ are constants (independent of $\\mathbf{w}$).\n",
    "***\n",
    "\n",
    "Although we can express $\\mathbf{w}^\\star$ explicitly in terms of the matrix inverse $(\\mathbf{X}^\\top \\mathbf{X})^{-1}$, this isn't an efficient way to compute $\\mathbf{w}$ numerically (we typically never compute the inverse of a matrix exactly when solving the system $A\\mathbf{x} = b$ for numerical stability). It is better instead to solve the following system of linear equations:\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}^\\star = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "***\n",
    "**Exercise:** Use `np.linalg.solve` to solve for $\\mathbf{w}^\\star$ using the single-featured training data.\n",
    "_Hint: You can enter `np.linalg.solve?` or `help(np.linalg.solve)` to see the docstring (help file)._\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepend a column of 1's to the design matrices (since we absorbed the bias term in the weights vector)\n",
    "X_train_b = np.column_stack((np.ones_like(X_train_s), X_train_s))\n",
    "X_test_b = np.column_stack((np.ones_like(X_test_s), X_test_s))\n",
    "print('Design matrix shape:', X_train_s.shape)\n",
    "\n",
    "w = ... # fill in\n",
    "print('Weights:', w)\n",
    "\n",
    "# Design matrix shape: (404, 1)\n",
    "# Weights: [34.51530004 -0.95801769]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check our implementation by plotting the predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"Return the predicted response for a given design matrix and weights vector\n",
    "    \"\"\"\n",
    "    return ... # fill in \n",
    "\n",
    "X_grid = np.linspace(X_train_s.min(), X_train_s.max(), num=1001)\n",
    "x = np.column_stack((np.ones_like(X_grid), X_grid))\n",
    "y = predict(x, w)\n",
    "plt.plot(X_grid, y, 'k-', label='Prediction')\n",
    "plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "#plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel(\"$y$ (Median House Value)\")\n",
    "plt.xlabel(\"$x$ (LSTAT)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll compute the mean error term over the training and test sets to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return ... # fill in \n",
    "\n",
    "y_pred_train = predict(X_train_b, w)\n",
    "y_pred_test = predict(X_test_b, w)\n",
    "print('Train MSE:', mean_squared_error(y_pred_train, y_train))\n",
    "print('Test MSE:', mean_squared_error(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. Linear regression using scikit-learn\n",
    "\n",
    "Now that you have a good understanding of what's going on under the hood, you can use the functionality in scikit-learn to solve linear regression problems you encounter in the future. Using the `LinearRegression` module, fitting a linear regression model becomes a one-liner as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `LinearRegression` module provides access to the bias weight $w_0$ under the `intercept_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and the non-bias weights under the `coef_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should check that these results match the solution you obtained previously. Note that sklearn also uses a numerical linear algebra solver under the hood.\n",
    "\n",
    "Finally, what happens if we use the other 12 features available in the dataset? To make predictions, we can utilize the `predict` API. Scikit-learn also includes an implementation of the `mean_squared_error` function that we can import and employ to assess our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr_full = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_train = lr_full.predict(X_train)\n",
    "y_pred_test = lr_full.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_pred_train, y_train))\n",
    "print('Test MSE:', mean_squared_error(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as sk_mse\n",
    "print('Train MSE:', sk_mse(y_pred_train, y_train))\n",
    "print('Test MSE:', sk_mse(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As measured by the MSE, our predictions are looking better. Although we obtained a performance boost here, in real problems you should be cautious of blindly including features in your analysis just because you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Basis expansion\n",
    "\n",
    "Linear regression is simple and easy to interpret, however it cannot capture nonlinear relationships between the response and features. \n",
    "To deal with this problem, we can extend linear regression by mapping the features into another space where the relationship is (ideally) linear. \n",
    "This is known as _basis expansion_.\n",
    "Specifically, we map the original feature vector $\\mathbf{x} \\in \\mathbb{R}^m$ to a new feature vector $\\varphi(\\mathbf{x}) \\in \\mathbb{R}^k$. \n",
    "We then perform linear regression as before, replacing the original feature vectors with the new feature vectors: $y = w_0 + \\sum_{i = 1}^{k} w_i \\cdot \\varphi_i(\\mathbf{x})$. \n",
    "Note that this function is nonlinear in $\\mathbf{x}$, but linear in $\\mathbf{w}$. \n",
    "\n",
    "All of the previous results for simple linear regression carry over as you would expect by making the replacement $\\mathbf{x} \\to \\varphi(\\mathbf{x})$. \n",
    "For instance, the normal equation becomes: \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\star = \\left[\\mathbf{\\Phi}^\\top \\mathbf{\\Phi}\\right]^{-1} \\mathbf{\\Phi}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Phi} = \\begin{pmatrix} \\varphi(\\mathbf{x}_1)^\\top \\\\ \\vdots \\\\ \\varphi(\\mathbf{x}_n)^\\top \\end{pmatrix}$ denotes the transformed design matrix.\n",
    "\n",
    "\n",
    "There are many possible choices for the mapping $\\varphi(\\mathbf{x})$, but we'll focus on using polynomial basis functions in the single-feature case, e.g. $\\varphi(x) = [1, x, x^2, \\ldots, x^{k - 1}]^\\top$ (note the first element corresponds to a bias term). \n",
    "\n",
    "We can compute the transformed design matrix using a built-in class from scikit-learn called `PolynomialFeatures`.\n",
    "We'll start by considering polynomial features of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "Phi_train = poly.fit_transform(X_train_s)\n",
    "Phi_test = poly.fit_transform(X_test_s)\n",
    "print(\"Original design matrix (first 5 rows):\\n\", X_train_s[0:5], \"\\n\")\n",
    "print(\"Transformed design matrix (first 5 rows):\\n\", Phi_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's perform linear regression on the transformed training data and plot the resulting model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr_poly = LinearRegression(fit_intercept=False).fit(Phi_train, y_train)\n",
    "\n",
    "X_grid = np.linspace(X_train_s.min(), X_train_s.max(), num=1001)\n",
    "Phi_grid = poly.fit_transform(X_grid[:,np.newaxis])\n",
    "y = lr_poly.predict(Phi_grid)\n",
    "plt.plot(X_grid, y, 'k-', label='Prediction')\n",
    "plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "#plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel(\"$y$ (Median House Value)\")\n",
    "plt.xlabel(\"$x$ (LSTAT)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Seems like a better fit than the linear model! Let's take a look at the error terms on the train/test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_train_poly = lr_poly.predict(Phi_train)\n",
    "y_pred_test_poly = lr_poly.predict(Phi_test)\n",
    "print('Train MSE for polynomial features of degree {}: {:.3f}'.format(degree, mean_squared_error(y_pred_train_poly, y_train)))\n",
    "print('Test MSE for polynomial features of degree {}: {:.3f}'.format(degree, mean_squared_error(y_pred_test_poly, y_test)))\n",
    "\n",
    "print('Train MSE using linear features only: {:.3f}'.format(mean_squared_error(lr.predict(X_train_s), y_train)))\n",
    "print('Test MSE using linear features only: {:.3f}'.format(mean_squared_error(lr.predict(X_test_s), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Strange, a large reduction on the train MSE but not so much on the test MSE. \n",
    "Lets scan across a range of powers. \n",
    "What do you expect to happen as we increase the maximum polynomial order on the training set? \n",
    "Take a minute to discuss with your fellow students before executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "degrees = list(range(12))\n",
    "models = list()\n",
    "train_mses = list()\n",
    "test_mses = list()\n",
    "\n",
    "X_grid = np.linspace(min(X_train_s.min(), X_test_s.min()), \n",
    "                     max(X_train_s.max(), X_test_s.max()), num=1001)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(len(degrees)//2, 2, i+1) \n",
    "    \n",
    "    # Transform features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    Phi_train, Phi_test = poly.fit_transform(X_train_s), poly.fit_transform(X_test_s)\n",
    "    Phi_grid = poly.fit_transform(X_grid[:,np.newaxis])\n",
    "    \n",
    "    # Fit model\n",
    "    lr_poly = LinearRegression().fit(Phi_train, y_train)\n",
    "    models.append(lr_poly)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = mean_squared_error(lr_poly.predict(Phi_train), y_train)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mse = mean_squared_error(lr_poly.predict(Phi_test), y_test)\n",
    "    test_mses.append(test_mse)\n",
    "    \n",
    "    plt.plot(X_grid, lr_poly.predict(Phi_grid), 'k', label='Prediction')\n",
    "    plt.scatter(X_train_s, y_train, color='b', marker='.', label='Train')\n",
    "    plt.scatter(X_test_s, y_test, color='r', marker='.', label='Test')\n",
    "    plt.title('Degree {} | Train MSE {:.3f}, Test MSE {:.3f}'.format(degree, train_mse, test_mse))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.suptitle('Polynomial regression for different polynomial degrees', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's plot mean-squared error vs. polynomial degree for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_mses, color='b', label='Train')\n",
    "plt.plot(degrees, test_mses, color='r', label='Test')\n",
    "plt.title('MSE vs. polynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "**Question**: ðŸ¤¨ What's going on here? Does this match your earlier findings, or your intuition about which model was most appropriate? Why isn't test error behaving the same as training error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "## Bonus: Ridge regression (this section is optional)\n",
    "\n",
    "One solution for managing the bias-variance trade-off is regularisation. \n",
    "In the context of regression, one can simply add a penalty term to the least-squares cost function in order to encourage weight vectors that are sparse and/or small in magnitude.\n",
    "In this section, we'll experiment with ridge regression, where a $L_2$ (Tikhonov) penalty term is added to the cost function as follows:\n",
    "\n",
    "$$\n",
    "C(\\mathbf{w}) = \\| \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\|_2^2 + \\alpha \\| \\mathbf{w} \\|_2^2\n",
    "$$\n",
    "\n",
    "***\n",
    "**Exercise:** Repeat the previous section on polynomial regression with an $L_2$ penalty term and $\\alpha = 0.002$. You may find the `sklearn.linear_model.Ridge` class useful.\n",
    "\n",
    "_Note: You'll need to rescale the `LSTAT` feature (e.g. divide by 100) in order to avoid numerical issues._\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}