{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Workshop 8: Recurrent Neural Networks\n",
    "\n",
    "\n",
    "This week we will be looking at recurrent neural networks, for processing sequential inputs. The workshop is based on the [Pytorch RNN Tutorial, by Sean Robertson](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). **Please start by reviewing the tutorial,** which builds a model for detecting nationality from a persons surname (a string), using a recurrent neural network. It has a lot more detailed descriptions of each of the steps, especially in terms of the use of the pytorch API.\n",
    "\n",
    "The code from the tutorial is replicated below, with a few modifications. In the workshop, we'll be moving from a simple RNN to a GRU (a more advanced variant, similar to the LSTM), and develop a model including attention. \n",
    "\n",
    "First, we will develop a RNN largely following the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, load the data files. We truncate the dataset during loading, so that the neural network training steps below run a little faster. \n",
    "\n",
    "**Before running the code below, you will first need to download the datafile from [here](https://download.pytorch.org/tutorial/data.zip) and extract it into the current directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    random.shuffle(lines)\n",
    "    lines = lines[:50] # prune the dataset to speed up training\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next step converts lines, which are strings of characters, into tensors. Please see the tutorial for an explanation of how each character is rendered as a 1-hot vector, and these are packaged together into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we implement the recurrent step in the RNN model. The idea is that we repeatedly call the `RNN.forward` function to unroll the network over a sequence. Each `hidden` state will be reused as input to the next call to the function. The final `output` vector will be used as part of the loss function.\n",
    "\n",
    "Compared to the PyTorch tutorial, we've added a non-linear activation function (a tanh) and changed the way in which the output layer is connected to the recurrent unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # changed input\n",
    "        self.activation = nn.Tanh() # new\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.activation(self.i2h(combined)) # changed to use activation\n",
    "        output = self.h2o(hidden) # changed input\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "We will now create a method `randomTrainingExample` which grabs a labelled instance from the training set, processes this into a tensor, and returns the result. We have added one wrinkle compared to the tutorial, namely we make the strings more complex by appending some punctuation characters. These are (mostly) meaningless, and the model can easily learn to skip over these inputs. However they will make learning more complex, and we will be assessing how robust training is when using different lengths of suffix, and how this changes with the use of the different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample(noise=0, noise_chars=\".,;'\"):\n",
    "    # noise: integer denoting the maximum number of distractor characters to add\n",
    "    # noise_chars: inventory of distractor characters\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    # added code to insert distracting nonsense into the string\n",
    "    if noise > 0:\n",
    "        line_prime = line\n",
    "        for i in range(random.randint(0, noise+1)):\n",
    "            line_prime += random.choice(noise_chars)\n",
    "        line = line_prime\n",
    "    # end change\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the new function to ensure it behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(5)\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice how instances are names with a short burst of punctuation appended to the end. This will form the training instances, and the category will serve as the target label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll now set the key variables for training. Note that we have added a `noise_level` variable, which we can use to increase the difficult of the dataset. Let's start with the clean data, that is with `noise_level=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_iters = 80000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "noise_level = 0 # change this line (as discussed later)\n",
    "n_hidden = 32\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The training loop below takes 2 minutes to complete 80000 SGD updates, so please be patient! You may be wondering about the training loop, which is a bit different to the normal presentation of SGD. Namely the training loop isn't structured into 'epochs' with data shuffling in each epoch, but instead proceeds a fixed number of updates with random sampling of data in each step. This is an alternative for the purpose of simplicity of exposition, and makes little difference to training. In practise, we tend to use the former method, with explicit epochs on real machine learning datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "rnn = RNNClassifier(n_letters, n_hidden, n_categories)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "\n",
    "# training algorithm, which takes one instance and performs single SGD update\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    # key step: unroll the RNN over each symbol in the input sequence\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    # treat the last output as the prediction of the category label\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    return output, loss.item()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now plot the loss values, which shows progress of training. Note that in most ML settings we care about generalisation error, and thus would look at performance on a heldout testing set. But for the purpose of this tutorial, we will focus on training, in terms of how efficiently RNN models can be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (Train)')\n",
    "plt.plot(range(0,n_iters,plot_every),all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## GRU recurrent unit\n",
    "\n",
    "Next we consider a more advanced hidden unit, namely the [\"gated recurrent unit\" or GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit). This unit includes a linear recurrent dynamic over the hidden state, which allows for better gradient behaviour when using back propagation. Namely there is less of an issue with gradient vanishing. This functions in largely a similar way to the long short term memory unit (LSTM), but is a little simpler and faster to compute.\n",
    "\n",
    "In the following, ensure you reset `noise_level=0`.\n",
    "\n",
    "First we will define a GRU classifier, which encodes an input sequence with a GRU, then uses the final hidden state to generate a multiclass output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "    def forward(self, input_sequence):\n",
    "        # apply GRU to full input sequence, and retain final hidden state\n",
    "        _, hidden = self.gru(input_sequence)\n",
    "        # couple final hidden state to multiclass classifier, i.e., softmax output\n",
    "        output = self.h2o(hidden.view(1, -1)) \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we are applying the GRU to full sequences, we will tinker with the training loop to accomodate this change. We will also use some more of the pytorch functionality, namely an explicit `SGD` optimiser object to encapsulate the gradient update routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = GRUClassifier(n_letters, n_hidden, n_categories)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses_gru = []\n",
    "current_loss = 0\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "\n",
    "    model.zero_grad()\n",
    "    #output = model(line_tensor)\n",
    "    output = model.forward(line_tensor)\n",
    "    output = torch.squeeze(output, 1) # remove redundant dimension\n",
    "    loss = criterion(output, category_tensor)\n",
    "    current_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses_gru.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (Train)')\n",
    "plt.plot(range(0,n_iters,plot_every),all_losses_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Attentional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, we will add an attentional component to the GRU model. This is designed to allow the use of hidden states besides the final state in parameterising the classifier. We will formulation attention as follows:\n",
    "\n",
    "\\begin{align}\n",
    "e_j & = f(\\mathbf{h}_j) \\\\\n",
    "\\alpha_j & = \\frac{\\exp(e_j)}{\\sum_k \\exp(e_k)} \\\\\n",
    "\\mathbf{c} & = \\sum_j \\alpha_j \\mathbf{h}_j\n",
    "\\end{align}\n",
    "\n",
    "where $f()$ is a linear function of its input vector, and the resulting $\\mathbf{c}$ is then used as input into the final linear classifier. Each component has the following role:\n",
    "- $\\mathbf{e}$: unnormalised attention scores, one value for each item in the input sequence\n",
    "- $\\mathbf{\\alpha}$: normalised attention scores, which can be interpreted as probabilities over the sequence of inputs (a.k.a. the *attention* vector)\n",
    "- $\\mathbf{c}$: a weighted combination of hidden states, based on the attention vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionalGRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionalGRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.att = nn.Linear(hidden_size, 1) \n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        # process the input sequence into a sequence of RNN hidden states\n",
    "        states, _ = self.gru(input_sequence)\n",
    "        # compute attention scores to each RNN hidden state (we use a linear function)\n",
    "        att_scores = self.att(states)\n",
    "        # rescale the attention scores using a softmax, so they sum to one\n",
    "        alpha = F.softmax(att_scores, dim=0)\n",
    "        # compute the \"c\" vector as a weighted combination of the RNN hidden states\n",
    "        c = torch.sum(torch.mul(states, alpha), dim=0)\n",
    "        # now couple up the c state to the output, and compute log-softmax\n",
    "        output = self.h2o(c.view(1, -1)) \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The training loop is the same as above, just using the new model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = AttentionalGRUClassifier(n_letters, n_hidden, n_categories)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses_att = []\n",
    "current_loss = 0\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "\n",
    "    model.zero_grad()\n",
    "    output, _ = model.forward(line_tensor)\n",
    "    output = torch.squeeze(output, 1) # remove redundant dimension\n",
    "    loss = criterion(output, category_tensor)\n",
    "    current_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses_att.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,n_iters,plot_every),all_losses, label='rnn')\n",
    "plt.plot(range(0,n_iters,plot_every),all_losses_gru, label='gru')\n",
    "plt.plot(range(0,n_iters,plot_every),all_losses_att, label='gru+attention')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (Train)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Discussion**: How do all three methods compare in terms of their efficiency of training? Is there a good empirical reason to use the GRU or attentional model over the vanilla RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now rerun all of the above models with `noise_level=5`. \n",
    "\n",
    "---\n",
    "**Discussion**: Are the loss values higher or lower after this change? Can you explain why?\n",
    "\n",
    "**Discussion**: Do your conclusions about the three models change, based on training on the noisy dataset? Why?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inspecting the Attention\n",
    "\n",
    "Finally, we can investigate how the attention is used. The code below shows some data instances and the computed attention vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "        output, attention = model.forward(line_tensor)\n",
    "        print(line, category, ['{:.2f}'.format(a) for a in attention.numpy().flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most of the attention is focussed on the last item in the sequence (when `noise_level=0`). \n",
    "\n",
    "**Question:** Why is this? And does this change when `noise_level=5`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this worksheet we have compared models in terms of their ease of training. However the more important test is in terms of generalisation accuracy. Typically when we test the more complex GRU and attentional models we see improvements in testing performance (ensuring they are all adequately trained, of course). Test the above three models by saving a separate set of unseen names from the original dataset to serve as a test set. Do you observe differences in accuracy?\n",
    "\n",
    "Next, try changing the GRU model above into a LSTM model. You can use `nn.LSTM` to do so, which supports a similar interface to `nn.GRU`. You will need to take special care with the hidden state, which has two components in the LSTM. Note that the LSTM can support several layers, although training may be much slower when using more than one layer. \n",
    "\n",
    "Efficient implementations of RNN models typically use much larger batches. Here we use a batch size of 1 for simplicity. But many pytorch operations can be applied over higher dimensional tensors to allow processing of several instances at once. For more insights into this, and other topics in state of the art RNN models, take a look at the tutorials in pytorch, including the one on [transformer models](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}